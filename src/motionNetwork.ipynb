{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import shutil\n",
    "from random import randint\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from skimage import io, color, exposure\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordInfo(logFile,data,header=None):\n",
    "    if os.path.exists(logFile):\n",
    "        with open(logFile, 'a') as log:\n",
    "            log.write(data)\n",
    "    else:\n",
    "        with open(logFile, 'w') as log:\n",
    "            log.write(header)\n",
    "            log.write(data)\n",
    "            \n",
    "def savePickle(name, toSave):\n",
    "    file = open(name, 'wb')\n",
    "    pickle.dump(toSave, file)\n",
    "    file.close()\n",
    "\n",
    "def loadPickle(name):\n",
    "    file = open(name, 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train and test videos 985 389\n",
      "985 389\n"
     ]
    }
   ],
   "source": [
    "class DataSplitter():\n",
    "    def __init__(self, path = 'Data/'):\n",
    "        self.path = path\n",
    "        self.actionLabel={}\n",
    "        with open(self.path+'classInd.txt') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\r\\n') for line in lines]\n",
    "        f.close()\n",
    "        for line in lines:\n",
    "            label,action = line.split(' ')\n",
    "            if action not in self.actionLabel.keys():\n",
    "                self.actionLabel[action]=label\n",
    "\n",
    "    def splitTestTrain(self):\n",
    "        self.trainVideo = self.file2_dic(self.path+'trainlist.txt')\n",
    "        self.testVideo = self.file2_dic(self.path+'testlist.txt')\n",
    "        print('number of train and test videos', len(self.trainVideo),len(self.testVideo))\n",
    "        return self.trainVideo, self.testVideo\n",
    "\n",
    "    def file2_dic(self,fname):\n",
    "        with open(fname) as f:\n",
    "            lines = f.readlines()\n",
    "            content = [line.strip('\\r\\n') for line in lines]\n",
    "        f.close()\n",
    "        dic={}\n",
    "        for line in lines:\n",
    "            video = line.split('/',1)[1].split(' ',1)[0]   # v_ApplyEyeMakeup_g01_c03.avi\n",
    "            key = video.split('_',1)[1].split('.',1)[0]    # ['v', 'ApplyEyeMakeup_g01_c03.avi'], ['ApplyEyeMakeup_g01_c03', 'avi']\n",
    "            label = self.actionLabel[line.split('/')[0]]   # 1\n",
    "            dic[key] = int(label)\n",
    "        return dic\n",
    "    \n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "splitter = DataSplitter()\n",
    "trainVideo,testVideo = splitter.splitTestTrain()\n",
    "print(len(trainVideo),len(testVideo))  # ApplyEyeMakeup_g08_c01': 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class motionDataset(Dataset):  \n",
    "    def __init__(self, dic, inChannel, rootDir = './Data', mode = 'train', transform=None):\n",
    "        #Generate a 16 Frame clip\n",
    "        self.keys=list(dic.keys())\n",
    "        self.values=list(dic.values())\n",
    "        self.rootDir = rootDir\n",
    "        self.transform = transform\n",
    "        self.mode=mode\n",
    "        self.inChannel = inChannel\n",
    "        self.imgRows=224\n",
    "        self.imgCols=224\n",
    "\n",
    "    def stackImages(self):\n",
    "        name = 'v_'+self.video\n",
    "        u = self.rootDir+ 'u/' + name\n",
    "        v = self.rootDir+ 'v/'+ name\n",
    "        \n",
    "        flow = torch.FloatTensor(2*self.inChannel,self.imgRows,self.imgCols)\n",
    "        i = int(self.clips_idx)\n",
    "\n",
    "\n",
    "        for j in range(self.inChannel):\n",
    "            idx = i + j\n",
    "            idx = str(idx)\n",
    "            frame_idx = 'frame'+ idx.zfill(6)\n",
    "            horiImage = u +'/' + frame_idx +'.jpg'\n",
    "            vertiImage = v +'/' + frame_idx +'.jpg'\n",
    "            \n",
    "            imgHori=(Image.open(horiImage))\n",
    "            imgVerti=(Image.open(vertiImage))\n",
    "\n",
    "            H = self.transform(imgHori)\n",
    "            V = self.transform(imgVerti)\n",
    "\n",
    "            \n",
    "            flow[2*(j-1),:,:] = H\n",
    "            flow[2*(j-1)+1,:,:] = V      \n",
    "            imgHori.close()\n",
    "            imgVerti.close()  \n",
    "        return flow\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.keys)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print ('mode:',self.mode,'calling Dataset:__getitem__ @ idx=%d'%idx)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            self.video, nb_clips = self.keys[idx].split('-')\n",
    "            self.clips_idx = random.randint(1,int(nb_clips))\n",
    "        elif self.mode == 'val':\n",
    "            self.video,self.clips_idx = self.keys[idx].split('-')\n",
    "        \n",
    "        label = self.values[idx]\n",
    "        label = int(label)-1 \n",
    "        data = self.stackImages()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            sample = (data,label)\n",
    "        elif self.mode == 'val':\n",
    "            sample = (self.video,data,label)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train and test videos 985 389\n",
      "==> Training data : 985  videos torch.Size([20, 224, 224])\n",
      "==> Validation data : 7391  frames torch.Size([20, 224, 224])\n",
      "124\n"
     ]
    }
   ],
   "source": [
    "class motionDataLoader():\n",
    "    def __init__(self,splitter, batchSize = 8, numWorkers = 0, inChannel = 10,  rootPath = './Data/'):\n",
    "        self.batchSize=batchSize\n",
    "        self.numWorkers = numWorkers\n",
    "        self.frame_count={}\n",
    "        self.inChannel = inChannel\n",
    "        self.dataPath=rootPath + 'tvl1_flow/'\n",
    "        # split the training and testing videos\n",
    "        splitter = splitter\n",
    "        self.trainVideo, self.testVideo = splitter.splitTestTrain()\n",
    "        with open(rootPath+'/frame_count.pickle','rb') as file:\n",
    "            dic_frame = pickle.load(file)\n",
    "        file.close()\n",
    "        for line in dic_frame :\n",
    "            videoname = line.split('_',1)[1].split('.',1)[0]\n",
    "            self.frame_count[videoname]=dic_frame[line] \n",
    "        \n",
    "    def getData(self):\n",
    "        trainLoader = self.getTrainLoader()\n",
    "        valLoader = self.getValLoader()\n",
    "        return trainLoader, valLoader, self.testVideo\n",
    "\n",
    "                            \n",
    "    def getTrainLoader(self):\n",
    "        self.dic_video_train={}\n",
    "        for video in self.trainVideo:\n",
    "            nb_clips = self.frame_count[video]-10+1\n",
    "            key = video +'-' + str(nb_clips)\n",
    "            self.dic_video_train[key] = self.trainVideo[video] \n",
    "        training_set = motionDataset(dic=self.dic_video_train, inChannel=self.inChannel, rootDir=self.dataPath,\n",
    "            mode='train',\n",
    "            transform = transforms.Compose([\n",
    "            transforms.Resize([224,224]),\n",
    "            transforms.ToTensor(),\n",
    "            ]))\n",
    "        print('==> Training data :',len(training_set),' videos',training_set[1][0].size())\n",
    "        trainLoader = DataLoader(\n",
    "            dataset=training_set, \n",
    "            batch_size=self.batchSize,\n",
    "            shuffle=True,\n",
    "            num_workers=self.numWorkers,\n",
    "            pin_memory=True\n",
    "            )\n",
    "        return trainLoader\n",
    "\n",
    "    def getValLoader(self):\n",
    "        self.dic_test_idx = {}\n",
    "        #print len(self.testVideo)\n",
    "        for video in self.testVideo:\n",
    "            n,g = video.split('_',1)\n",
    "\n",
    "            sampling_interval = int((self.frame_count[video]-10+1)/19)\n",
    "            for index in range(19):\n",
    "                clip_idx = index*sampling_interval\n",
    "                key = video + '-' + str(clip_idx+1)\n",
    "                self.dic_test_idx[key] = self.testVideo[video]\n",
    "        validation_set = motionDataset(dic= self.dic_test_idx, inChannel=self.inChannel, rootDir=self.dataPath ,\n",
    "            mode ='val',\n",
    "            transform = transforms.Compose([\n",
    "            transforms.Resize([224,224]),\n",
    "            transforms.ToTensor(),\n",
    "            ]))\n",
    "        print('==> Validation data :',len(validation_set),' frames',validation_set[1][1].size())\n",
    "        #print validation_set[1]\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            dataset=validation_set, \n",
    "            batch_size=self.batchSize, \n",
    "            shuffle=False,\n",
    "            num_workers=self.numWorkers)\n",
    "        return val_loader\n",
    "    \n",
    "dataloader = motionDataLoader(splitter)\n",
    "trainLoader,valLoader,testVideo = dataloader.getData()\n",
    "print(len(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_transform(modelDict, pretrainedDict, channel):\n",
    "    weightDict  = {k:v for k, v in pretrainedDict.items() if k in modelDict}\n",
    "    w3 = pretrainedDict['conv1.weight']\n",
    "    if channel == 3:\n",
    "        wt = w3\n",
    "    else:\n",
    "        S=0\n",
    "        for i in range(3):\n",
    "            S += w3[:,i,:,:]\n",
    "        avg = S/3.\n",
    "        newW3 = torch.FloatTensor(64,channel,7,7)\n",
    "        for i in range(channel):\n",
    "            newW3[:,i,:,:] = avg.data\n",
    "        wt = newW3\n",
    "    weightDict['conv1_custom.weight'] = wt\n",
    "    modelDict.update(weightDict)\n",
    "    return modelDict\n",
    "\n",
    "\n",
    "def conv3x3(inChannels, outChannels, stride=1):\n",
    "    return nn.Conv2d(inChannels, outChannels, kernel_size=3, stride=stride,padding=1, bias=False)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, nb_classes=10, channel=20):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1_custom = nn.Conv2d(channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self.makeLayer(block, 64, layers[0])\n",
    "        self.layer2 = self.makeLayer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self.makeLayer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self.makeLayer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "        self.fc_custom = nn.Linear(512 * block.expansion, nb_classes)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def makeLayer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(nn.Conv2d(self.inplanes, planes * block.expansion,kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes * block.expansion),)\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1_custom(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc_custom(x)\n",
    "        return out\n",
    "    \n",
    "def resnet34(pretrained=False, channel= 3, **kwargs):\n",
    "    model = ResNet(ResidualBlock, [3, 4, 6, 3], nb_classes=10, channel=channel, **kwargs)\n",
    "    if pretrained:\n",
    "       pretrainedDict = model_zoo.load_url('https://download.pytorch.org/models/resnet34-333f7ec4.pth')                 \n",
    "       modelDict = model.state_dict()\n",
    "       modelDict =weight_transform(modelDict, pretrainedDict, channel)\n",
    "       model.load_state_dict(modelDict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionCNN():\n",
    "    def __init__(self, lr, trainLoader, testLoader,test_video,resume = 'motionModel', endEpochs = 200, batchSize =8,channel = 10*2):\n",
    "        self.endEpochs=endEpochs\n",
    "        self.lr=lr\n",
    "        self.batchSize=batchSize\n",
    "        self.resume=resume\n",
    "        self.startEpoch=0\n",
    "        self.trainLoader=trainLoader\n",
    "        self.testLoader=testLoader\n",
    "        self.best_prec1=0\n",
    "        self.channel=channel\n",
    "        self.test_video=test_video\n",
    "        self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        #build model\n",
    "        self.model = resnet34(pretrained= True, channel=self.channel).to(device)\n",
    "        #print self.model\n",
    "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), self.lr, momentum=0.9)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=1,verbose=True)\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.epoch=0\n",
    "        prec1, val_loss = self.validate_1epoch()\n",
    "        return prec1, val_loss\n",
    "    \n",
    "    def train(self):\n",
    "        if self.resume:\n",
    "            if os.path.isfile(self.resume):\n",
    "                checkpoint = torch.load(self.resume)\n",
    "                self.start_epoch = checkpoint['epoch'] + 1\n",
    "                self.best_prec1 = checkpoint['best_prec1']\n",
    "                self.model.load_state_dict(checkpoint['state_dict'])\n",
    "                self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "                print(\"==> loaded checkpoint '{}' (epoch {}) (best_prec1 {})\"\n",
    "                  .format(self.resume, checkpoint['epoch'], self.best_prec1))\n",
    "            else:\n",
    "                print(\"==> no checkpoint found at '{}'\".format(self.resume))\n",
    "        cudnn.benchmark = True\n",
    "        \n",
    "        with tqdm(total = self.endEpochs * len(self.trainLoader)) as pbar:\n",
    "            for self.epoch in range(self.startEpoch, self.endEpochs):\n",
    "                print('==> Epoch:[{0}/{1}][training stage]'.format(self.epoch, self.endEpochs))\n",
    "                batch_time = AverageMeter()\n",
    "                data_time = AverageMeter()\n",
    "                losses = AverageMeter()\n",
    "                top1 = AverageMeter()\n",
    "                top5 = AverageMeter()\n",
    "                #switch to train mode\n",
    "                self.model.train()    \n",
    "                end = time.time()\n",
    "                # mini-batch training\n",
    "                for i, (data,label) in enumerate(self.trainLoader):\n",
    "                    pbar.update(1)\n",
    "                    # measure data loading time\n",
    "                    data_time.update(time.time() - end)\n",
    "\n",
    "                    label = label.to(device)\n",
    "                    input_var = Variable(data).to(device)\n",
    "                    target_var = Variable(label).to(device)\n",
    "\n",
    "                    # compute output\n",
    "                    output = self.model(input_var)\n",
    "                    loss = self.criterion(output, target_var)\n",
    "\n",
    "                    # measure accuracy and record loss\n",
    "                    prec1, prec5 = self.accuracy(output.data, label, topk=(1, 5))\n",
    "                    losses.update(loss.item(), data.size(0))\n",
    "                    top1.update(prec1.item(), data.size(0))\n",
    "                    top5.update(prec5.item(), data.size(0))\n",
    "\n",
    "                    # compute gradient and do SGD step\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                    # measure elapsed time\n",
    "                    batch_time.update(time.time() - end)\n",
    "                    end = time.time()\n",
    "                header = 'Epoch,Loss,Prec@1,Prec@5\\n'\n",
    "                info = '{0},{1},{2},{3}\\n'.format(self.epoch,round(losses.avg,5),round(top1.avg,4),round(top5.avg,4))\n",
    "                recordInfo('motionTrainingLog.txt', info, header)\n",
    "                print(header,info)\n",
    "                \n",
    "\n",
    "                if self.epoch % 1 == 0:\n",
    "                    prec1, val_loss = self.validate()\n",
    "                    is_best = prec1 > self.best_prec1\n",
    "                    #lr_scheduler\n",
    "                    self.scheduler.step(val_loss)\n",
    "                    # save model\n",
    "                    if is_best:\n",
    "                        self.best_prec1 = prec1\n",
    "                        with open('motionVideoPreds','wb') as f:\n",
    "                            pickle.dump(self.dictToVideoLevelPreds,f)\n",
    "                        f.close() \n",
    "                torch.save({ 'epoch': self.epoch, 'state_dict': self.model.state_dict(),'best_prec1': self.best_prec1, 'optimizer' : self.optimizer.state_dict() }, self.resume)\n",
    "\n",
    "       \n",
    "    def validate(self):\n",
    "        print('==> Epoch:[{0}/{1}][validation stage]'.format(self.epoch, self.endEpochs))\n",
    "        batch_time = AverageMeter()\n",
    "        losses = AverageMeter()\n",
    "        top1 = AverageMeter()\n",
    "        top5 = AverageMeter()\n",
    "        self.model.eval()\n",
    "        self.dictToVideoLevelPreds={}\n",
    "        end = time.time()\n",
    "        with tqdm(total =  len(self.testLoader)) as pbar:\n",
    "            for i, (keys,data,label) in enumerate(self.testLoader):\n",
    "                pbar.update(1)\n",
    "                label = label.to(device)\n",
    "                with torch.no_grad():\n",
    "                    data_var = Variable(data).to(device)\n",
    "                    label_var = Variable(label ).to(device)\n",
    "                output = self.model(data_var)\n",
    "                batch_time.update(time.time() - end)\n",
    "                end = time.time()\n",
    "                preds = output.data.cpu().numpy()\n",
    "                nb_data = preds.shape[0]\n",
    "                for j in range(nb_data):\n",
    "                    videoName = keys[j].split('-',1)[0] # ApplyMakeup_g01_c01\n",
    "                    if videoName not in self.dictToVideoLevelPreds.keys():\n",
    "                        self.dictToVideoLevelPreds[videoName] = preds[j,:]\n",
    "                    else:\n",
    "                        self.dictToVideoLevelPreds[videoName] += preds[j,:]\n",
    "\n",
    "        #Frame to video level accuracy\n",
    "        video_top1, video_top5, video_loss = self.frameToVideoLevelAccuracy()\n",
    "        header = 'Epoch,Loss,Prec@1,Prec@5\\n'\n",
    "        info = '{0},{1},{2},{3}\\n'.format(self.epoch,np.round(video_loss,5),np.round(video_top1,3),np.round(video_top5,3))\n",
    "        recordInfo('motionValLog.txt', info, header)\n",
    "        print(header,info)\n",
    "        return video_top1, video_loss\n",
    "    \n",
    "    def accuracy(self,output, target, topk=(1,)):\n",
    "        maxk = max(topk)\n",
    "        batchSize = target.size(0)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0)\n",
    "            res.append(correct_k.mul_(100.0 / batchSize))\n",
    "        return res\n",
    "    \n",
    "\n",
    "    def frameToVideoLevelAccuracy(self):\n",
    "        correct = 0\n",
    "        videoLevelPreds = np.zeros((len(self.dictToVideoLevelPreds),10))\n",
    "        videoLevelLabels = np.zeros(len(self.dictToVideoLevelPreds))\n",
    "        ii=0\n",
    "        for key in sorted(self.dictToVideoLevelPreds.keys()):\n",
    "            name = key.split('-',1)[0]\n",
    "\n",
    "            preds = self.dictToVideoLevelPreds[name]\n",
    "            label = int(self.test_video[name])-1\n",
    "                \n",
    "            videoLevelPreds[ii,:] = preds\n",
    "            videoLevelLabels[ii] = label\n",
    "            ii+=1         \n",
    "            if np.argmax(preds) == (label):\n",
    "                correct+=1\n",
    "\n",
    "        #top1 top5\n",
    "        videoLevelLabels = torch.from_numpy(videoLevelLabels).long()\n",
    "        videoLevelPreds = torch.from_numpy(videoLevelPreds).float()\n",
    "\n",
    "        loss = self.criterion(Variable(videoLevelPreds).to(device), Variable(videoLevelLabels).to(device))    \n",
    "        top1,top5 = self.accuracy(videoLevelPreds, videoLevelLabels, topk=(1,5))     \n",
    "                            \n",
    "        top1 = float(top1.numpy())\n",
    "        top5 = float(top5.numpy())\n",
    "            \n",
    "        return top1,top5,loss.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Training data : 985  videos torch.Size([20, 224, 224])\n",
      "==> Validation data : 7391  frames torch.Size([20, 224, 224])\n",
      "==> Epoch:[0/200][validation stage]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306d7e8e9326406aab1b3213ede28c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=924), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch,Loss,Prec@1,Prec@5\n",
      " 0,9.24275016784668,12.339,49.871\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12.33933162689209, array(9.242753, dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader, valLoader, test_video = dataloader.getData()\n",
    "model = MotionCNN(lr=1e-2,trainLoader=trainLoader, testLoader=valLoader,test_video=test_video)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
